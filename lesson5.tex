%!TEX root = main.tex
\chapter{Lineare Algebra, Vektoren und Matrizen}
\todo{Probabilistische system entwicklung, https://www.youtube.com/watch?v=K-8F\_zDMDUI\&list=PLMrJAkhIeNNTYaOnVI3QpH7jgULnAmvPA\&index=3}
\todo[inline]{Digital Audio als vektor, Vector scope/oscilloscope music, stereo sachen, Rotationsmatrix, Hadamardmatrix, Householdermatrix,  state space formulations, modal analyse}

\citep{strang2020linear}

% \begin{multicols}{2}

\section{Motivation}
Vektoren und Matrizen findet man sehr häufig in der Audiotechnischen Literatur. Wie bereits in Kapitel \ref{chap:complex} beschrieben, sind komplexe Zahlen allgegenwärtig und deren Interpretation als Vektor bedeutsam und praktisch. Andererseits kann zB. ein Stereosignal als Vektor aufgefasst werden: Zu jedem Zeitpunkt 'hat' es 2 Zahlen, diese können als Vektor im 2-dimensionalen Raum aufgefasst werden, was wiederum diverse Operationen nahelegt. Ebenso scheint es spätestens an diesem Punkt naheliegend ein $n$-Kanal Signal als Vektor mit $n$ Einträgen zu verstehen und womöglich hieraus Schlüsse ziehen zu dürfen. Wenn es um Verräumlichung von Signalen (zB. Ambisonics) geht und daher Positionen im Dreidimensionalen Raum ($\mathbb{R}^3$) benötigt werden ist klar, dass wir diese mittels Vektoren beschreiben können. Aber auch wenn es um Direktionalität von Lautsprechern oder Mikrophonen geht werden letztlich 2 bis 3 Dimensionale Felder beschrieben und auch hier kommen Vektoren zum Einsatz. Es sei aber an dieser Stelle betont dass Vektoren und Matrizen nicht ausschließlich für Audioanwendungen im Dreidimensionalen Raum zum Einsatz kommen. Rotations- Hadamard- und Householdermatrizen \citep{PASPWEB2010}\footnote{https://ccrma.stanford.edu/~jos/pasp/Hadamard\_Matrix.html} werden für Hall-Algorithmen und Resonatoren verwendet , Filtersysteme können über Zustandsraumdarstellungen via Vektoren und Matrizen beschrieben werden \citep{bilbao2009numerical} und vieles mehr.       

\section{Vektoren}
Es wird nun einführendes über Vektoren gesagt aber recht kurz und bündig da 1. angenommen wird, dass insbesondere 2 und 3 dimensionale Vektoren zu den weniger abstrakten und aus der Schule bekannten Themen gehören und 2. Vektoren als Spezialfall von Matrizen gesehen werden können und wir unsere Aufmerksamkeit so schnell wie möglich diesen zuwenden wollen.

\important{
Ein Vektor in einem $n$-Dimensionalen Raum ($\mathbb{R}^n$) ist eine geordnete Liste von Zahlen.

\begin{equation}
\vec{a} = \begin{bmatrix}
a_1 & a_2 & \dots & a_n
\end{bmatrix}
\end{equation}


Der Vektor $\vec{a}$ und seine Elemente $a_1, a_2, \dots a_n$ sind hier als \textbf{Reihenvektor} (auch 'Zeilenvektor', engl.'Row vector') dargestellt. Ebenso können wir \textbf{Spaltenvektoren} (engl. 'column vector') darstellen:
\begin{equation}
\vec{a} = \begin{bmatrix}
a_1 \\
a_2 \\
\vdots \\
a_n
\end{bmatrix}
\end{equation}

\textbf{Konventionen} \\
Dieses Dokument wird die Konvention 'Reihen' statt 'Zeilen' verwenden. Vorteilhaft ist daran, dass 'R' vor 'S' im Alphabet kommt und es hilft uns uns die \emph{Reihenfolge} von Indizes von Matrizen zu merken: \textbf{Reihe, Spalte}.

Es gibt unterschiedliche Schreibweisen in Unterschiedlicher Literatur. Dieses Dokument verwendet die Schreibweise $\vec{a}$. Manchmal findet man auch fett gedruckte Buchstaben um zu notieren, dass es sich um einen Vektor handelt: $\mathbf{a}$. Zudem gibt es Literatur die runde Klammern bevorzugt statt eckigen wie in diesem Dokument: $\mathbf{a} = \begin{pmatrix}
a_1 & a_2 & \dots & a_n
\end{pmatrix}$. All das ist lediglich ein Unterschied in Konventionen.
}


\begin{python}{Vektoren: Arrays in Python}
In [1]: %pylab
In [2]: a = array([1,2,3]) #Vektor Definition
   ...: print(a.shape)
   ...: print(a)
(3,)
[1 2 3]
In [3]: b = array([[5, 6, 7]]) # Matrix, 1 Reihenvektor
   ...: print(b.shape)
   ...: print(b)
(1, 3)
[[5 6 7]]
In [4]: c = array([[8, 9, 10]]).T # Matrix, 1 Spaltenvektor
   ...: print(c.shape)
   ...: print(c)
(3, 1)
[[ 8]
 [ 9]
 [10]]
\end{python}

\subsection*{Hadamard\footnote{Jacques Hadamard (Jacques Salomon Hadamard; * 8. Dezember 1865 in Versailles; † 17. Oktober 1963 in Paris) war ein französischer Mathematiker.} Produkt}
\index{Hadamard Produkt}

Auch Elementweises oder Komponentenweises Produkt. Einfache elementweise Multiplikation.
$$ \vec{a}\circ \vec{b} = \begin{bmatrix}a_1b_1 & a_2b_2 & \dots & a_n b_n\end{bmatrix}$$ 

Leider wird üblicherweise die selbe Notation benutzt um die Verkettung von Funktionen zu beschreiben: $(f \circ g)(x) = f(g(x))$. Welche Operation nun gemeint ist, erschließt sich typischerweise aus dem Kontext. Beide Notationen kommen in unserem Kontext ohnehin nicht wahnsinnig oft vor.

In Python\footnote{Hier ist ein wichtiger unterschied zu \texttt{MATLAB} Code da dort punktweise Multiplikation mit \texttt{.*} ausgeführt wird.} kann elementweise Multiplikation durch einfache Multiplikation ausgeführt werden: \pythoninline{c = a*b}.

\begin{python}{Elementweise Multiplikation von 2 Arrays in Python.}
In [1]: %pylab
In [2]: a = array([1,2,3])
In [3]: b = array([5, 6, 7])
In [4]: a*b
Out[4]: array([ 5, 12, 21])
\end{python}

\begin{question}
In Python, definiere 2 Arrays, $\vec{a}$ und $\vec{b}$. Diese sollen jeweils eine Sinus Schwingung enthalten, aber jeweils mit anderer Frequenz. Multipliziere sie, plotte das Ergebnis und höre dir das Ergebnis an. Befehle um Arrays als Audio file auf die Festplatte zu schreiben sind im Python Cheat-Sheet zu finden in Abschnitt \ref{sec:pythonCheat}. Was passiert hier aus Audio Sicht?
\end{question}

\begin{answer}
Zunächst, was passiert ist eine 'Ringmodulation' (sozusagen eine Variante der Amplitudenmodulation). Um eine Sinus Schwingung mit Frequenz $f_a$ definieren wir eine Funktion $a(t)$:
\begin{equation}
a(t) = sin(2 \pi t f_a)
\end{equation}
Um hier eine Sekunde Sound zu erzeugen mit Samplingrate $f_s = 44100$ benötigen wir einen Vektor $\vec{t}$ der die Zeit in Sekunden enthält: $\vec{t}:= \begin{bmatrix} 0 & \Delta_t& 2 \Delta_t &  \dots 1-\Delta_t \end{bmatrix}$ wobei $\Delta_t$ das Sampling-Intervall $\Delta_t = \frac{1}{f_s}$. Praktisch ist es am einfachsten man erzeugt man einen Array der einen Index enthält: $ns := \begin{bmatrix} 0 & 1 & 2 &  N-1 \end{bmatrix}$. Dies kann man in python via \texttt{arange(N)} erzielen. Diese kann anschliessend durch die samplingrate dividiert werden um $\vec{t}$ zu erhalten. Da wir eine Sekunde Audio erzeugen wollen ist $N=f_s$ die Samplingrate.


\begin{python}{Ringmodulation.}
In [1]: %pylab
In [2]: import soundfile as sf
In [3]: sr= 44100
In [4]: ns = arange(sr)
In [5]: t = ns/sr
In [6]: a = sin(2*pi*t*100)
In [7]: b = sin(2*pi*t*50)
In [8]: c = a*b
In [9]: plot(t,c)
In [10]: sf.write('ringmodTest.wav', c, sr)
\end{python}
\end{answer}


\subsection*{Skalarprodukt}
\index{Skalarprodukt}

Auch 'inneres Produkt' oder 'Punktprodukt', engl. 'Dot product'. Ergibt im fall von Vektoren einen Skalar aber eigentlich ein Spezialfall der Matrixmultiplikation!
$$\vec{a}\cdot \vec{b} = |\vec{a}|\cdot |\vec{b}|\cdot cos \sphericalangle ({\vec {a}},{\vec {b}}) = a_1 b_1 + a_2 b_2 + \dots +a_n b_n = \sum_{i=1}^N a_ib_i$$
Siehe \citep[p.~45]{Westermmann2008}  für Herleitung.

Der Teil $|\vec{a}|\cdot |\vec{b}|\cdot cos \sphericalangle ({\vec {a}},{\vec {b}})$ mag weniger intuitiv sein als die summe der Produkte, mag auch weniger Praktisch sein in der Anwendung. Aus diesem Teil ergibt sich aber eine entscheidende Eigenschaft des Skalarprodukts: \emph{Es ist 0 wenn die Vektoren Senkrecht aufeinander stehen.}

Python: \pythoninline{dot(a,b)}

\begin{python}{Skalarprodukt in Python}
In [1]: %pylab
In [2]: a = array([2,3,4])
In [3]: b = array([5,6,7])
In [4]: 2*5+3*6+4*7
Out[4]: 56
In [5]: dot(a,b)
Out[5]: 56
\end{python}

% \end{multicols}

\praxis{
    Hat all dies irgendeine Relevanz in der Audio-Verarbeitung?
    Eine einfache Bezugnahme kann hergestellt werden indem wir uns verdeutlichen dass die Energie eines zeitdiskreten Signals \citep{christensen2019introduction} folgendermaßen berechnet werden kann:
    $$ E = \sum_{n=0}^{N-1} |x[n]| ^2$$
    \index{Energie}
    Also wir nehmen den Betrag von allen Elementen\footnote{Man mag sich fragen wozu man den Betrag nimmt um anschließend zu quadrieren. Nach dem quadrieren kann doch ohnehin kein negatives Vorzeichen mehr erscheinen. Diese Definition lässt jedoch auch komplexe Signale zu!}, quadrieren sie alle und summieren sie. Dies ist im Falle eines Realwertigen Signals das selbe wie das Skalare Produkt des Vektors mit sich selbst ($aa = a^2$). \index{RMS}\index{Quadratisches Mittel} Die Formel kann ein wenig modifiziert werden um uns den 'Root Mean Squared' (RMS, das Quadratische Mittel) zu berechnen:
    $$ RMS = \sqrt{\frac{1}{N}\sum^N_{i=1} x_i^2}$$
    \index{Korrelation}
    Eine Weiteres Beispiel aus der Praxis ist in Abbildung \ref{fig:dotProdCorr} veranschaulicht. Die in den Titeln der Plots ersichtlichen Skalaren Produkte legen nahe dass hier vielleicht ein Normalisierungsfaktor fehlt um den Korrelationsgrad zu errechnen.
    Abbildung \ref{fig:dotProdCorr} wurde durch dieses Notebook erstellt: \colab{https://colab.research.google.com/github/hrtlacek/matheFuerTonmeisterinnen/blob/master/python/notebooks/dotProduct.ipynb}.



\begin{figure}[H]
    \centering
    \input{python/plots/dotProd.pgf}
    \caption{Das Skalare Produkt unterschiedlicher Signale.}
    \label{fig:dotProdCorr}
\end{figure}


}




\subsection*{Kreuzprodukt}
\index{Kreuzprodukt}



\todo[inline]{TODO}
Auch 'Vektorprodukt'. 
$$ \vec{c} = \vec{a} \times \vec{b}$$
Nur in $\mathbb{R}^3$ definiert!


\begin{equation}
{\displaystyle {\vec {a}}\times {\vec {b}}={\begin{pmatrix}a_{1}\\a_{2}\\a_{3}\end{pmatrix}}\times {\begin{pmatrix}b_{1}\\b_{2}\\b_{3}\end{pmatrix}}={\begin{pmatrix}a_{2}b_{3}-a_{3}b_{2}\\a_{3}b_{1}-a_{1}b_{3}\\a_{1}b_{2}-a_{2}b_{1}\end{pmatrix}}\,.}
\end{equation}

Python: \pythoninline{cross(a,b)}
\begin{python}{Demonstration des Kreuzprodukts}
In [1]: %pylab
In [2]: a = array([1,0,0])
In [3]: b = array([0,1,0])
In [4]: cross(a,b)
Out[4]: array([0, 0, 1])
\end{python}



\section{Matrizen}



Eine Matrix ist eine rechteckige Struktur von Zahlen:


\begin{equation}
\mathbf{A} = \begin{bmatrix} a_{11} & a_{12} & \dots & a_{1m} \\
                    a_{21} & a_{22} &       &  \vdots   \\
                    \vdots &        &\ddots &  \vdots \\
                    a_{n 1} & \dots & \dots & a_{n m}
 \end{bmatrix}
\end{equation}



\praxis{
Angenommen wir wollen $4$ Audio Signale, zum Beispiel 4 verschiedene Instrumente auf 6 verschiedene Lautsprecher schicken, unterschiedlicher Lautstärke. Wie könnte man dies Mathematisch formulieren? Man könnte zum Beispiel sagen die 4 Audio Signale seien ein Vektor $\vec{x(t)}$, der die jeweiligen Signale enthält: $\vec{x}(t) = \begin{bmatrix} x_1(t) & x_2(t) & x_3(t) & x_4(t)\end{bmatrix}$. Unsere Output Signale die wir an die 6 Lautsprecher schicken nennen wir vielleicht $y_1(t), \dots, y_6(t)$. Auch diese machen wir zu einem Vektor $\vec{y}(t) = \begin{bmatrix} y_1(t) & y_2(t) & y_3(t) & y_4(t) & y_5(t) & y_6(t)\end{bmatrix}$. Nun können wir unsere Verräumlichung über eine Matrix definieren:

$$ \mathbf{A}\vec{x} = \vec{y}$$

Hier ist zu beachten dass es sich bei $\vec{x}$ um eine $4\times 1$ Matrix, also einen \textbf{Spaltenvektor} mit 4 Reihen handelt, und bei $\vec{y}$ ebenso um einen Spaltenvektor. Die Matrix $\mathbf{A}$ hat die Form $6 \times 4$. zB:

\[
\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 \end{bmatrix}  
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4  \end{bmatrix}  
=
\begin{bmatrix} y_1 \\ y_2 \\ y_3\\ y_4\\ y_5\\ y_6\end{bmatrix}
\]


In diesem Beispiel haben wir beschlossen Instrument $1$ auf Lautsprecher $1$ zu schicken, Instrument $2$ auf Lautsprecher $2$ etc, aber Instrument $1$ auch noch auf Lautsprecher 5 und 6 zu senden. Das heißt: $y_1 = x_1$, $y_2 = x_2$, $y_3 = x_3$, $y_4 = x_4$, $y_5 = x_1$, $y_6 = x_1$.

Ein colab notebook das dies in python demonstriert kann hier gefunden werden: \colab{https://colab.research.google.com/github/hrtlacek/matheFuerTonmeisterinnen/blob/master/python/notebooks/spatialization\_matrix.ipynb}
}




\subsection*{Ein Lineares Gleichungssystem (LGS)}

Gegeben folgendes Gleichungssystem:

\[
\begin{aligned}
2x + 3y &= 8 \\
4x -  y &= 2
\end{aligned}
\]

\subsubsection{Versuch einer Algebraischen Lösung via Sympy}
Bevor wir den Zusammenhang mit Vektoren und Matrizen erkunden sei darauf hingewiesen, dass wir \emph{meistens}\footnote{siehe: \href{https://docs.sympy.org/latest/guides/solving/solve-system-of-equations-algebraically.html}{https://docs.sympy.org/latest/guides/solving/solve-system-of-equations-algebraically.html}} das System via \texttt{sympy} lösen können: 
\begin{python}{Algebraisches Lösen eines Gleichungssytems.}
In [1]: import sympy as sp
In [2]: from sympy.abc import x,y,z,t
In [3]: sp.init_printing()
In [4]: g1 = sp.Eq(2*x + 3*y, 8)
In [5]: g2 = sp.Eq(4*x - y, 2)
In [6]: g1
Out[6]: 2⋅x + 3⋅y = 8
In [7]: g2
Out[7]: 4⋅x - y = 2
In [8]: sp.solve([g1, g2], [x,y])
Out[8]: {x: 1, y: 2}
\end{python}

\subsubsection{Geometrische Interpretation: Zeileninterpretation}
 Wir wollen eine Lösung für $x$ und $y$ finden. Aus der Schule kennen wir vielleicht eine geometrische Interpretation des Problems. Wir können beide Gleichungen für $y$ lösen und bekommen zwei neue Gleichungen. Beide beschreiben eine Linie. Die Lösung besteht in deren Schnittpunkt:

\[
\begin{aligned}
y &= \frac{8}{3} - \frac{2 x}{3} \\
y &= 4 x - 2 
\end{aligned}
\]


\begin{figure}[H]
    \centering
    \input{python/plots/lgs_geom.pgf}
    \caption{Geometrische Interpretation des Gleichungssystems.}
    \label{fig:lgs_geom}
\end{figure}

Obiger Plot und Lösungen berechnet mit diesem Notebook: \colab{https://colab.research.google.com/github/hrtlacek/matheFuerTonmeisterinnen/blob/master/python/notebooks/lgs\_geo.ipynb}.


\subsubsection{Geometrische Interpretation: 'Spalteninterpretation'}
Wir können das obige System umschreiben zu:
$$
x \color{codeRed} \begin{bmatrix} 2 \\ 4 \end{bmatrix}  
 \color{black} +  y \color{codeGreen} \begin{bmatrix} 3 \\ -1 \end{bmatrix}  
\color{black} =
\color{codeBlue} \begin{bmatrix} 8 \\ 2 \end{bmatrix} $$

Die Frage wird dann zu: 'Wie viel des Vektors $\begin{bmatrix} 2 \\ 4 \end{bmatrix}$ und wie viel des Vektors $\begin{bmatrix} 3 \\ -1 \end{bmatrix}$ muss ich nehmen um den Vektor $\begin{bmatrix} 8 \\ 2 \end{bmatrix}$ zu erhalten?'. Wir kennen die Antwort schon: 1 mal den ersten und zwei mal den zweiten. Dies ist verdeutlicht in Abbildung \ref{fig:lgs_geom_col}. 



\begin{figure}[H]
    \centering
    \input{python/plots/lgsGeomColumn.pgf}
    \caption{Geometrische Interpretation des Gleichungssystems: Linearkombination der Spalten.}
    \label{fig:lgs_geom_col}
\end{figure}

Es handelt sich also um eine \emph{Linearkombination der Spalten} der Matrix: $$\mathbf{A} = \begin{bmatrix} 2 & 3 \\ 4 & -1 \end{bmatrix}$$





\subsubsection{LGS in Matrizenschreibweise}


Dies kann in Matrixform geschrieben werden als:

\[
\begin{bmatrix} 2 & 3 \\ 4 & -1 \end{bmatrix}  
\begin{bmatrix} x \\ y \end{bmatrix}  
=
\begin{bmatrix} 8 \\ 2 \end{bmatrix}
\]


\important{Die Situation des Gleichungssystems oben kann allgemeiner als $$\mathbf{A}\vec{x} = \vec{b}$$ zusammengefasst werden. Und das wiederum kann konzeptualisiert werden als Linearkombination der Spalten in $\mathbf{A}$ mit den Faktoren in $\vec{x}$ mit dem Ergebnis $\vec{b}$.
}



\begin{python}{Numerische Lösung eines Linearen Gleichungssystems mit Python.}
In [1]: %pylab
In [2]: A = np.array([[2, 3], [4, -1]])
In [3]: b = np.array([8, 2])
In [4]: x = np.linalg.solve(A, b)
In [5]: x
Out[5]: array([1., 2.])
\end{python}








\subsection{Spezielle Matrizen}

\subsubsection*{Null Matrix}
Eine fett notierte 0, $\mathbf{0}$ notiert eine mit nullen gefüllte Matrix.
\begin{equation}
\mathbf{0} =
\begin{bmatrix}
0 & 0 & 0 & \cdots & 0 \\
0 & 0 & 0 & \cdots & 0 \\
0 & 0 & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 0
\end{bmatrix}
\end{equation} 

In python können wir eine solche Matrix leicht erzeugen:
\begin{python}{Nullmatrix in python. 2 Reihen, 3 Spalten.}
In [1]: %pylab
In [2]: zeros([2,3])
Out[2]:
array([[0., 0., 0.],
       [0., 0., 0.]])
\end{python}

Mathematisch wirkt diese Matrix zunächst nicht wahnsinnig hilfreich, in Python kommt dies aber sehr oft vor. Beispielsweise wenn wir einfach einen Array initialisieren wollen um ihn anschließend in einer \texttt{for}-Schleife zu befüllen. \footnote{Das angeführte Beispiel ist ein wenig konstruiert. Wir wissen, wir können das hier erzielte Ergebnis einfacher erreichen: \pythoninline{sin(arange(10))}. Die Struktur des Beispiels wird hilfreicher wenn wir zum Beispiel Filter berechnen oder dynamische Systeme simulieren, siehe Abschnitt \ref{sub:dynamische_systeme}.}

\begin{python}{Array initialisieren und via \texttt{for}-Schleife befüllen.}
In [1]: a = zeros(10)
In [2]: for i in range(10):
   ...:     a[i] = sin(i)
In [3]: a
Out[3]:
array([ 0.        ,  0.84147098,  0.90929743,  0.14112001, -0.7568025 ,
       -0.95892427, -0.2794155 ,  0.6569866 ,  0.98935825,  0.41211849])
\end{python}

\subsubsection*{Einheitsmatrix}

$\mathbf{E}$ in deutschen Texten, in englisch 'Identity Matrix', daher $\mathbf{I}$. Dieser Text hat sich für $\mathbf{I}$ entschieden.

\begin{equation}
\mathbf{I} =
\begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
\end{equation} 


\begin{python}{Numerische $3\times 3$ Einheitsmatrix.}
In [1]: %pylab
In [2]: A = eye(3)
In [3]: A
Out[3]:
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
\end{python}

% \mathbf{I} = \begin{bmatrix} 1 & 0 & \dots & 0 \\
%                     0 & 1 & \dots &  \vdots   \\
%                     \vdots &  \vdots  &1&  0 \\
%                     0 & 0 & \dots & 1
%  \end{bmatrix}

\subsubsection{Diagonalmatrix}
Die Einheitsmatrix ist ein Spezialfall einer Diagonalmatrix:
% \begin{equation}
 $$ \mathbf{D}=\operatorname {diag} (d_{1},d_{2},\dotsc ,d_{n}):={\begin{bmatrix}d_{1}&0&\cdots &0\\0&d_{2}&\ddots &\vdots \\\vdots &\ddots &\ddots &0\\0&\cdots &0&d_{n}\end{bmatrix}} $$

% \end{equation}

\begin{python}{Erstellung einer Diagonalmatrix in Python.}
In [1]: diag([1,2,3,4])
Out[1]:
array([[1, 0, 0, 0],
       [0, 2, 0, 0],
       [0, 0, 3, 0],
       [0, 0, 0, 4]])
\end{python}

\begin{itemize}
   \item Quadratisch
   \item Eigenwerte sind in der Diagonale abzulesen
   \item Effizienter invertierbar
   \item Skaliert Vektorräume.
   \item Die Determinante ist das Produkt der Einträge in der Diagonale.
\end{itemize}




Beliebige Matrizen können unter gewissen Bedingungen \emph{diagonalisiert} werden. Das heißt es kann eine sog. \emph{ähnliche} Matrix gefunden werden die diagonal ist.
\todo[inline]{Kommentar zu oder Definition von 'ähnlich'}
\begin{python}{diagonalisierung}
import sympy as sp  
M = sp.Matrix([[3, -2,  4, -2], 
               [5,  3, -3, -2], 
               [5, -2,  2, -2], 
               [5, -2, -3,  3]]) 
In [21]: _,D = M.diagonalize()
In [22]: D
Out[22]:
⎡-2  0  0  0⎤
⎢           ⎥
⎢0   3  0  0⎥
⎢           ⎥
⎢0   0  5  0⎥
⎢           ⎥
⎣0   0  0  5⎦
\end{python}

Alternativ können die eigenwerte berechnet werden via \texttt{eig()} und daraus eine diagonalmatrix gemacht werden:
\begin{python}{diagonalisierung via \texttt{eig()}}
import sympy as sp  
M = sp.Matrix([[3, -2,  4, -2], 
               [5,  3, -3, -2], 
               [5, -2,  2, -2], 
               [5, -2, -3,  3]]) 
In [21]: _,D = M.diagonalize()
In [22]: D
Out[22]:
⎡-2  0  0  0⎤
⎢           ⎥
⎢0   3  0  0⎥
⎢           ⎥
⎢0   0  5  0⎥
⎢           ⎥
⎣0   0  0  5⎦
\end{python}


\subsubsection{Symmetrische Matrizen}

$\mathbf{A} \, ist \, symmetrisch \iff für alle i,j, a_{ji} = a_{ij}$

$\mathbf{A} = \mathbf{A}^T$



\subsubsection{Orthogonale Matrizen}
(Rotationsmatrizen)

Ist eine Matrix $\mathbf{Q}$ orthogonal dann gilt 
$$ \mathbf{Q}^T\mathbf{Q} = \mathbf{I} $$.


In Abbildung \ref{fig:rotMat} wurden zufällig gestreute Punkte um $20^\circ$ \emph{um den Ursprung} gedreht. In diesem Notebook kann dies nachvollzogen werden: 
\colab{https://colab.research.google.com/github/hrtlacek/matheFuerTonmeisterinnen/blob/master/python/notebooks/rotationMatrix.ipynb}

Die geschah mit der Rotationsmatrix:


\begin{equation}
    {\displaystyle \mathbf{A}={\begin{bmatrix}\cos \theta &-\sin \theta \\\sin \theta &\cos \theta \end{bmatrix}}}
\end{equation}

Wobei $$\theta = 2\cdot \pi \cdot W / 360$$ und $W$ der Winkel in Grad.

\begin{python}{Orthogonalität einer Rotationsmatrix}
deg = 20
θ = 2*π*deg/360 
A = array([[cos(θ), -sin(θ)],
           [sin(θ), cos(θ)]])
#check for orthogonality:
A.T@A
Out[1]: array([[1.00000000e+00, 2.22218033e-17],
       [2.22218033e-17, 1.00000000e+00]])

\end{python}



\begin{figure}[H]
    \centering
    \input{python/plots/rotMat.pgf}
    \caption{Anwendung einer Rotationsmatrix auf zufällig gestreute Punkte.}
    \label{fig:rotMat}
\end{figure}





% Hier  https://www.youtube.com/watch?v=j8hEnyOiwhw wird folgendes behauptet:
% $$ \mathbf{Q} \mathbf{Q}^T \neq \mathbf{I} $$.
% Auf wikipedia steht das gegenteil.

\subsection*{Matrizenmultiplikation}


Matrizenmultiplikation ist \emph{nicht kommutativ}(!). Das heißt allgemein gilt:
\begin{equation}
    \mathbf{AB} \neq \mathbf{BA}
\end{equation}

Es ist auch nicht die beliebige Multiplikation zweier Matrizen möglich. Bei der Multiplikation von zwei Matrizen $\mathbf{A}$ und $\mathbf{B}$ also $\mathbf{AB}$ hat das Resultat die Anzahl an Reihen wie $\mathbf{A}$ und die Anzahl an Spalten wie $\mathbf{B}$. 

Bei der Multiplikation von $\mathbf{AB}$ müssen die jeweils anderen Dimensionen übereinstimmen, das heißt die Anzahl der Spalten von $\mathbf{A}$ muss gleich der Anzahl von Reihen von $\mathbf{B}$ sein. Siehe Abbildung \ref{fig:matrixMultDims}.


\begin{figure}[H]
\centering
\begin{tikzpicture}


\matrix [draw=codeRed,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth](A)
  {
    \node {}; & \node{}; & \node {}; \\
    \node {}; & \node{}; & \node {}; \\
    \node {}; & \node{}; & \node {}; \\
    \node {}; & \node{}; & \node {}; \\
    \node {}; & \node{}; & \node {}; \\
  };

\matrix [draw=codeBlue,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth, right=1cm of A](B)
  {
    \node {}; & \node{}; & \node {}; & \node {};\\
    \node {}; & \node{}; & \node {}; & \node {}; \\
    \node {}; & \node{}; & \node {}; & \node {}; \\
  };


\matrix [draw=codeGreen,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth, right=1cm of B](C)
  {
    \node {}; & \node{}; & \node {}; & \node {}; \\
    \node {}; & \node{}; & \node {}; & \node {}; \\
    \node {}; & \node{}; & \node {}; & \node {}; \\
    \node {}; & \node{}; & \node {}; & \node {}; \\
    \node {}; & \node{}; & \node {}; & \node {}; \\
  };

    % Multiplication sign
    \node at ($(A.east)!0.5!(B.west)$) {$\cdot$};
    \node at ($(B.east)!0.5!(C.west)$) {$=$};


\end{tikzpicture}
\caption{Visualisierung der Matrizenmultiplikation}
    \label{fig:matrixMultDims}
\end{figure}

Die multiplikation zweier Matrizen ist formal definiert als:

\begin{equation}
{\displaystyle c_{ik}=\sum _{j=1}^{n}a_{ij}\cdot b_{jk}}
\end{equation}


\subsubsection{Beispiel}

Wir wollen Matrizen $\mathbf{A}$ und $\mathbf{B}$ multiplizieren und damit $\mathbf{C}$ berechnen:

\begin{equation}
\mathbf{AB}  = \mathbf{C}
\end{equation}

\begin{equation}
\mathbf{A}  = \left[\begin{matrix}1 & 2 & 3 \\ 4 & 5 & 6\end{matrix}\right]
\end{equation}

\begin{equation}
\mathbf{B}  = \left[\begin{matrix}1 & 2\\3 & 4\\5 & 6\end{matrix}\right]
\end{equation}


$\mathbf{A}$ ist eine $2 \times 3$ Matrix, $\mathbf{A}$ ist eine $3 \times 2$ Matrix. $\mathbf{A}$ bestimmt die Anzahl der Reihen von $\mathbf{C}$ und $\mathbf{B}$ die Anzahl der Spalten. $\mathbf{C}$ ist somit eine $2 \times 2$ Matrix.




\begin{figure}[H]
\centering
\begin{tikzpicture}


\matrix [draw=codeRed,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth](A)
  {
    \node {1}; & \node{2}; & \node {3}; \\
    \node {4}; & \node{5}; & \node {6}; \\
  };

\matrix [draw=codeBlue,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth, right=1cm of A](B)
  {
    \node {1}; & \node{2}; \\
    \node {3}; & \node{4}; \\
    \node {5}; & \node{6}; \\
  };


\matrix [draw=codeGreen,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth, right=1cm of B](C)
  {
    \node {$c_{11}$}; & \node{$c_{12}$}; \\
    \node {$c_{21}$}; & \node{$c_{22}$}; \\
  };

    % Multiplication sign
    \node at ($(A.east)!0.5!(B.west)$) {$\cdot$};
    \node at ($(B.east)!0.5!(C.west)$) {$=$};


\end{tikzpicture}
\caption{Visualisierung der Matrizenmultiplikation}
    \label{fig:matrixMultExample}
\end{figure}




Um $c_{11}$ also den Wert in der ersten Reihe und ersten SPalte zu berechnen multiplizieren wir die erste Reihe von A mit der ersten Spalte von B und bilden die Summe. Wir bilden sozusagen das \emph{Skalarprodukt} des Vektors in Reihe 1 von $\mathbf{A}$ mit dem Vektor in Spalte 1 von $\mathbf{B}$:

\begin{equation}
    c_{11} = 1 \cdot 1 + 2 \cdot 3 + 3 \cdot 5 = 22
\end{equation}

\begin{figure}[H]
\centering
\begin{tikzpicture}


\matrix [draw=codeRed,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth](A)
  {
    \node[fill=codeRed] {1}; & \node[fill=codeRed]{2}; & \node[fill=codeRed] {3}; \\
    \node {4}; & \node{5}; & \node {6}; \\
  };

\matrix [draw=codeBlue,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth, right=1cm of A](B)
  {
    \node[fill=codeBlue] {1}; & \node{2}; \\
    \node[fill=codeBlue] {3}; & \node{4}; \\
    \node[fill=codeBlue] {5}; & \node{6}; \\
  };


\matrix [draw=codeGreen,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth, right=1cm of B](C)
  {
    \node [fill=codeGreen]{22}; & \node{}; \\
    \node {}; & \node{}; \\
  };

    % Multiplication sign
    \node at ($(A.east)!0.5!(B.west)$) {$\cdot$};
    \node at ($(B.east)!0.5!(C.west)$) {$=$};


\end{tikzpicture}
\caption{Visualisierung der Matrizenmultiplikation}
    \label{fig:matrixMultColored}
\end{figure}

Das selbe passiert mit allen anderen Einträgen der Matrix $\mathbf{C}$, hier noch $c_{22}$ berechnet:

\begin{equation}
    c_{22} = 4 \cdot 2 + 5 \cdot 4 + 6 \cdot 6 = 64
\end{equation}

\begin{figure}[H]
\centering
\begin{tikzpicture}


\matrix [draw=codeRed,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth](A)
  {
    \node {1}; & \node{2}; & \node {3}; \\
    \node[fill=codeRed] {4}; & \node[fill=codeRed]{5}; & \node[fill=codeRed] {6}; \\
  };

\matrix [draw=codeBlue,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth, right=1cm of A](B)
  {
    \node{1}; & \node[fill=codeBlue]{2}; \\
    \node{3}; & \node[fill=codeBlue]{4}; \\
    \node{5}; & \node[fill=codeBlue]{6}; \\
  };


\matrix [draw=codeGreen,column sep=0.5cm, 
    nodes={draw=black, minimum size=0.5cm, anchor=center},column sep=-\pgflinewidth, row sep=-\pgflinewidth, right=1cm of B](C)
  {
    \node {22}; & \node{}; \\
    \node {}; & \node[fill=codeGreen]{64}; \\
  };

    % Multiplication sign
    \node at ($(A.east)!0.5!(B.west)$) {$\cdot$};
    \node at ($(B.east)!0.5!(C.west)$) {$=$};


\end{tikzpicture}
\caption{Visualisierung der Matrizenmultiplikation}
    \label{fig:matrixMultColored}
\end{figure}



Wenn wir das alles durchgespielt haben kommen wir letztlich zu unserem Ergebnis
\begin{equation}
\mathbf{C} = \left[\begin{matrix}22 & 28\\49 & 64\end{matrix}\right]
\end{equation}


Selbstverständlich können wir all das in Python erledigen:

\begin{python}{Matrizenmultiplikation in Python.}
In [1]: %pylab
In [2]: import sympy as sp
In [3]: sp.init_printing()
In [4]: A = array([[1,2,3],
   ...:             [4,5,6]])
In [5]: B = array([[1,2],
   ...: [3,4],
   ...: [5,6]])
In [6]: C = A@B
In [7]: sp.Matrix(C) # nur zur huebschen anzeige in dem Fall.
Out[7]:
⎡22  28⎤
⎢      ⎥
⎣49  64⎦
In [8]: C #nicht so huebsche Anzeige.
Out[8]:
array([[22, 28],
       [49, 64]])
\end{python}

Weiters können wir uns davon überzeugen dass die Matrixmultiplikation eben nicht kommutativ ist:
\begin{python}{Demonstration der nicht-Kommutativität der Matrizenmultiplikation.}
In [9]: D = B@A
In [10]: sp.Matrix(D)
Out[10]:
⎡9   12  15⎤
⎢          ⎥
⎢19  26  33⎥
⎢          ⎥
⎣29  40  51⎦
\end{python}


\subsection*{Transposition}
\subsection*{Determinante}
\todo[inline]{A Test for invertibility}
$det \mathbf{A}$ oder auch $|\mathbf{A}|$. 

\begin{itemize}
   \item $det \mathbf{A} = 1$.
   \item Reihen vertauschen kehrt das vorzeichen der Determinante um.
\end{itemize}

$$ \begin{vmatrix} a & b  \\
                    c & d 
 \end{vmatrix} = ad - bc$$

\colab{https://colab.research.google.com/github/hrtlacek/matheFuerTonmeisterinnen/blob/master/python/notebooks/Determinante.ipynb}


\subsection*{Invertierung}

\subsubsection{Diagonalisierung}
 % https://www.youtube.com/watch?v=ZSGrJBS_qtc


\subsection*{Eigenwert und Eigenvektoren}
\index{Eigenwert}\index{Eigenvektor}\index{Zustandsraumdarstellung}\index{State-Space}

\praxis{Eigenwerte der $A$ Matrix in der Zustandsraumdarstellung (engl. 'State-Space') von Systemen sind die Pole des Systems. Eigenvektoren bei der Modalanalyse können Modenformen (die eigentliche Bewegung eines Systems zB.) darstellen und in Eigenwerten sind die entsprechenden Frequenzen ($\Im(\lambda)$) und die Stabilität ($\Re(\lambda)$) enthalten. Eigenvektoren von Matrizen die für Probabilistische Modellierung (siehe Markov Modelle) können Stationäre Wahrscheinlichkeits-Vektoren sein.}


\subsection*{Dynamische Systeme}\label{sub:dynamische_systeme}

\section{Aufgaben}
\begin{enumerate}
\item \todo[inline]{Aufgabe: Stereo/Vectorscope aus Oszilloscop bauen. Rotation d Vektors um 45 grad. }
\end{enumerate}
